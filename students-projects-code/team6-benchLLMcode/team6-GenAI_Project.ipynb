{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm_PJlWY-nrr",
        "outputId": "0d314cf4-c7c0-420d-824a-2ef0252a5bfa",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.50.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai tqdm anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import subprocess\n",
        "import tempfile\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "HXZtcAMgBp1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "anthropic_client = OpenAI(api_key=userdata.get('ANTHROPIC_API_KEY'), base_url=\"https://api.anthropic.com/v1/\")\n",
        "xai_client = OpenAI(api_key=userdata.get(\"XAI_API_KEY\"), base_url=\"https://api.x.ai/v1\")"
      ],
      "metadata": {
        "id": "jovp81iyCF10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUMANEVAL_PATH = \"HumanEvalPlus.jsonl\"\n",
        "if not os.path.exists(HUMANEVAL_PATH):\n",
        "    raise FileNotFoundError(f\"{HUMANEVAL_PATH} not found\")"
      ],
      "metadata": {
        "id": "FZ-g_I1SCVh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_humaneval_data(file_path=HUMANEVAL_PATH):\n",
        "    problems = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            problems.append(json.loads(line))\n",
        "    print(f\"Loaded {len(problems)} problems from {file_path}\")\n",
        "    return problems"
      ],
      "metadata": {
        "id": "mYjupU48CYeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "def _seconds_from_msg(msg):\n",
        "    match = re.search(r'Please try again in ([\\dhms]+)\\.', msg)\n",
        "    if not match:\n",
        "        return 30\n",
        "\n",
        "    total = 0\n",
        "    for value, unit in re.findall(r'(\\d+)([hms])', match.group(1)):\n",
        "        n = int(value)\n",
        "        total += n * {'h': 3600, 'm': 60, 's': 1}[unit]\n",
        "    return total or 30\n",
        "\n",
        "def generate_code(model_type, prompt, max_tokens=1024, temperature=1.0, max_retries=3):\n",
        "    model_map = {\n",
        "        \"o3\": (\"o3\", openai_client),\n",
        "        \"claude\": (\"claude-3-7-sonnet-20250219\", anthropic_client),\n",
        "        \"grok\": (\"grok-3-latest\", xai_client),\n",
        "    }\n",
        "\n",
        "    if model_type not in model_map:\n",
        "        raise ValueError(f\"Unknown model: {model_type}\")\n",
        "\n",
        "    model, client = model_map[model_type]\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": (\n",
        "                            \"You are an AI assistant that generates Python code. \"\n",
        "                            \"Only provide the code implementation without any explanations or comments. \"\n",
        "                            \"Make sure your code includes all imports and is compilable.\"\n",
        "                        ),\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"Complete the following Python function:\\n\\n{prompt}\",\n",
        "                    },\n",
        "                ],\n",
        "                max_completion_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                n=1,\n",
        "            )\n",
        "            return completion.choices[0].message.content\n",
        "\n",
        "        except Exception as e:\n",
        "            err_msg = str(e)\n",
        "            if \"code: 429\" in err_msg and \"rate limit\" in err_msg.lower():\n",
        "                wait_s = _seconds_from_msg(err_msg)\n",
        "                print(\n",
        "                    f\"[attempt {attempt}/{max_retries}] \"\n",
        "                    f\"Rate-limit hit. Sleeping {wait_s} s before retrying…\"\n",
        "                )\n",
        "                time.sleep(wait_s)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"Error generating code: {err_msg}\")\n",
        "                return None\n",
        "\n",
        "    print(\"Error generating code: maximum retries exceeded.\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "P7P24LNpDRUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_code(response):\n",
        "    if response is None:\n",
        "        return None\n",
        "\n",
        "    if \"```python\" in response and \"```\" in response:\n",
        "        code_blocks = response.split(\"```\")\n",
        "        for i, block in enumerate(code_blocks):\n",
        "            if i % 2 == 1:\n",
        "                if block.startswith(\"python\"):\n",
        "                    block = block[6:]\n",
        "                return block.strip()\n",
        "\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "Uq56rvs0C8fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import signal, traceback\n",
        "\n",
        "def execute_solution(problem, solution, timeout=10.0):\n",
        "    def _time_out(signum, frame):\n",
        "        raise TimeoutError(\"execution timed out\")\n",
        "    signal.signal(signal.SIGALRM, _time_out)\n",
        "    signal.alarm(int(timeout))\n",
        "\n",
        "    ns: dict = {}\n",
        "    err = None\n",
        "    passed = False\n",
        "\n",
        "    try:\n",
        "        exec(solution, ns)\n",
        "        ns[\"candidate\"] = ns[problem[\"entry_point\"]]\n",
        "        exec(problem[\"test\"], ns)\n",
        "        ns[\"check\"](ns[\"candidate\"])\n",
        "        passed = True\n",
        "    except Exception as e:\n",
        "        err = traceback.format_exc()\n",
        "    finally:\n",
        "        signal.alarm(0)\n",
        "    return passed, err"
      ],
      "metadata": {
        "id": "lE57sRfUU6G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_problem(problem, model):\n",
        "    task_id = problem['task_id']\n",
        "    print(f\"Processing {task_id}...\")\n",
        "\n",
        "    prompt = problem[\"prompt\"]\n",
        "    response = generate_code(model, prompt)\n",
        "    code = extract_code(response)\n",
        "\n",
        "    passed, error = execute_solution(problem, code)\n",
        "\n",
        "    return {\n",
        "        'task_id': task_id,\n",
        "        'passed': passed,\n",
        "        'error': error,\n",
        "        'generated_code': code\n",
        "    }"
      ],
      "metadata": {
        "id": "o3L7EH42DxOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_benchmark(problems, model, problem_delay=1):\n",
        "    print(f\"Processing {len(problems)} problems for {model}\")\n",
        "\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for problem in tqdm(problems, desc=f\"Running {model}\"):\n",
        "        result = process_problem(problem, model)\n",
        "        results.append(result)\n",
        "        if problem_delay > 0:\n",
        "            time.sleep(problem_delay)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    pass_count = sum(1 for r in results if r['passed'])\n",
        "    pass_at_1 = pass_count / len(problems)\n",
        "\n",
        "    print(f\"\\nBenchmark for {model} completed in {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Pass@1: {pass_at_1:.4f} ({pass_count}/{len(problems)})\")\n",
        "\n",
        "    results_file = f'{model}_humaneval_results.json'\n",
        "    with open(results_file, 'w') as f:\n",
        "        json.dump({\n",
        "            'model': model,\n",
        "            'pass_at_1': pass_at_1,\n",
        "            'num_problems': len(problems),\n",
        "            'elapsed_time': elapsed_time,\n",
        "            'results': results\n",
        "        }, f, indent=2)\n",
        "\n",
        "    print(f\"Results saved to {results_file}\")\n",
        "\n",
        "    return results, pass_at_1, elapsed_time"
      ],
      "metadata": {
        "id": "RyiEIXm8EGfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_full_benchmark(problems, models=['claude', 'grok', 'o3']):\n",
        "    results = {}\n",
        "\n",
        "    for model in models:\n",
        "        if os.path.isfile(f'{model}_humaneval_results.json'):\n",
        "            continue\n",
        "        print(f\"\\n=== Running benchmark for {model} ===\\n\")\n",
        "        _, pass_rate, elapsed_time = run_benchmark(problems, model)\n",
        "        results[model] = {'pass_rate': pass_rate, 'elapsed_time': elapsed_time}\n",
        "\n",
        "    model_names = list(results.keys())\n",
        "    pass_rates = [results[model]['pass_rate'] * 100 for model in model_names]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(model_names, pass_rates)\n",
        "\n",
        "    for bar, rate in zip(bars, pass_rates):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f\"{rate:.1f}%\", ha='center', va='bottom')\n",
        "\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Pass Rate (%)')\n",
        "    plt.title('HumanEval Pass@1 Results by Model')\n",
        "    plt.ylim(0, 110)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.savefig('model_comparison.png')\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Model': model_names,\n",
        "        'Pass Rate (%)': [f\"{results[model]['pass_rate'] * 100:.2f}%\" for model in model_names],\n",
        "        'Elapsed Time (s)': [f\"{results[model]['elapsed_time']:.2f}\" for model in model_names]\n",
        "    })\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZUJjMAYLUuFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problems = load_humaneval_data()\n",
        "results_df = run_full_benchmark(problems)\n",
        "\n",
        "print(\"\\n=== Benchmark Results ===\\n\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "results_df.to_csv('benchmark_results.csv', index=False)\n",
        "print(\"\\nResults saved to benchmark_results.csv\")"
      ],
      "metadata": {
        "id": "re8UfObFZfj9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5e8792e-8251-42fe-8def-ad2428b28042"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 164 problems from HumanEvalPlus.jsonl\n",
            "\n",
            "=== Running benchmark for o3 ===\n",
            "\n",
            "Processing 164 problems for o3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   0%|          | 0/164 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/0...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   1%|          | 1/164 [00:06<18:17,  6.73s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/1...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   1%|          | 2/164 [00:19<27:16, 10.10s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/2...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   2%|▏         | 3/164 [00:30<29:04, 10.83s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/3...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   2%|▏         | 4/164 [00:33<20:31,  7.70s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/4...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   3%|▎         | 5/164 [00:58<37:06, 14.00s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/5...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   4%|▎         | 6/164 [01:23<46:20, 17.60s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/6...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   4%|▍         | 7/164 [01:29<35:50, 13.70s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/7...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   5%|▍         | 8/164 [01:54<44:53, 17.27s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/8...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   5%|▌         | 9/164 [02:20<51:49, 20.06s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/9...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   6%|▌         | 10/164 [02:47<56:47, 22.13s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/10...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   7%|▋         | 11/164 [02:56<46:42, 18.32s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/11...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   7%|▋         | 12/164 [03:26<55:12, 21.79s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/12...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   8%|▊         | 13/164 [03:32<42:49, 17.02s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/13...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   9%|▊         | 14/164 [04:00<50:52, 20.35s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/14...\n",
            "[attempt 1/3] Rate-limit hit. Sleeping 432 s before retrying…\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:   9%|▉         | 15/164 [11:38<6:18:17, 152.33s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/15...\n",
            "[attempt 1/3] Rate-limit hit. Sleeping 20 s before retrying…\n",
            "[attempt 2/3] Rate-limit hit. Sleeping 432 s before retrying…\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:  10%|▉         | 16/164 [19:47<10:25:50, 253.72s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/16...\n",
            "[attempt 1/3] Rate-limit hit. Sleeping 20 s before retrying…\n",
            "[attempt 2/3] Rate-limit hit. Sleeping 432 s before retrying…\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:  10%|█         | 17/164 [27:49<13:09:13, 322.14s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/17...\n",
            "[attempt 1/3] Rate-limit hit. Sleeping 20 s before retrying…\n",
            "[attempt 2/3] Rate-limit hit. Sleeping 432 s before retrying…\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:  11%|█         | 18/164 [35:51<15:00:54, 370.23s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/18...\n",
            "[attempt 1/3] Rate-limit hit. Sleeping 20 s before retrying…\n",
            "[attempt 2/3] Rate-limit hit. Sleeping 432 s before retrying…\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rRunning o3:  12%|█▏        | 19/164 [43:54<16:16:32, 404.09s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing HumanEval/19...\n",
            "[attempt 1/3] Rate-limit hit. Sleeping 20 s before retrying…\n",
            "[attempt 2/3] Rate-limit hit. Sleeping 432 s before retrying…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning o3:  12%|█▏        | 20/164 [51:57<17:06:25, 427.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing HumanEval/20...\n",
            "[attempt 1/3] Rate-limit hit. Sleeping 20 s before retrying…\n",
            "[attempt 2/3] Rate-limit hit. Sleeping 432 s before retrying…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning o3:  13%|█▎        | 21/164 [1:00:01<17:39:43, 444.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing HumanEval/21...\n",
            "[attempt 1/3] Rate-limit hit. Sleeping 20 s before retrying…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning o3:  13%|█▎        | 22/164 [1:00:29<12:36:53, 319.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing HumanEval/22...\n",
            "[attempt 1/3] Rate-limit hit. Sleeping 432 s before retrying…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning o3:  13%|█▎        | 22/164 [1:01:58<6:40:03, 169.04s/it] \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-127f6a6850e3>\u001b[0m in \u001b[0;36mgenerate_code\u001b[0;34m(model_type, prompt, max_tokens, temperature, max_retries)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for o3 in organization org-jjLLOJFZ3M1BaLSNxBlOjbfs on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-338d02565926>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mproblems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_humaneval_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_full_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Benchmark Results ===\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-05dce2efbeb6>\u001b[0m in \u001b[0;36mrun_full_benchmark\u001b[0;34m(problems, models)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n=== Running benchmark for {model} ===\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'pass_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpass_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'elapsed_time'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-6562cea6422c>\u001b[0m in \u001b[0;36mrun_benchmark\u001b[0;34m(problems, model, problem_delay)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mproblem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Running {model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_problem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproblem_delay\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-0c405eab7123>\u001b[0m in \u001b[0;36mprocess_problem\u001b[0;34m(problem, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-127f6a6850e3>\u001b[0m in \u001b[0;36mgenerate_code\u001b[0;34m(model_type, prompt, max_tokens, temperature, max_retries)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0;34mf\"Rate-limit hit. Sleeping {wait_s} s before retrying…\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 )\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}